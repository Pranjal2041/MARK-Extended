{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARK - Extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we re-implemented [MARK Model](https://openreview.net/pdf?id=sFyrGPCKQJC) from scratch. We first showed recreated the results on Cifar-100 dataset, and also highlighted possible issues in the paper. We then implemented the model on ImageData, followed by MarketCL dataset. We showed the effectiveness of MARK on such complex datasets. This was followed by incorporating SupSup Framework in order to allow scalability of model to much larger number of tasks, while being efficient in terms of model size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we show a demo training and testing code for MarketCL Model. Relevant flags can be adjusted to run various variations. Whole code is available in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and PreProcess the datasets\n",
    "!python prepare_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models_marketcl import MARKLSTMModel\n",
    "from trainer_market import test_loop\n",
    "from trainer_market import train_fe, train_kb_nonmeta, train_mg_n_clf, update_kb, train_kb_nonmeta_baseline\n",
    "from dataloaders import get_marketcl_dataloader\n",
    "from advanced_logger import LogPriority\n",
    "import pickle\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenter Class\n",
    "For Creating Code Backups, Logging and config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: MarketExperiment7\n",
      "Experiment Description: Market Experiment with larger FE SupSup Disabled\n",
      "Config File: configs/market1.yml\n",
      "[23:30:25]: Experiment started\n"
     ]
    }
   ],
   "source": [
    "from experimenter import Experimenter\n",
    "cfg_file = 'configs/market1.yml'\n",
    "experimenter = Experimenter(cfg_file)\n",
    "cfg = experimenter.config\n",
    "print = experimenter.logger.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_marketcl_dataloader(batch_size = cfg.TRAINING.BATCH_SIZE) # Train dl\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "model = MARKLSTMModel(cfg, device, 0)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Train Loop\n",
    "# At the End of the day, it evaluates the model on next days data as well.\n",
    "# Once training for all symbols on that day is done, testing on all symbols is performed to \n",
    "# get backward transfers and final accuracies.\n",
    "def train(model : MARKLSTMModel, criterion, train_dl, experimenter, device, baseline, iter):\n",
    "\n",
    "\n",
    "    print = experimenter.logger.log\n",
    "\n",
    "    # There are four steps:\n",
    "    # 1. Train the feature extractor on given task.\n",
    "    # 2. Train KB if task = 0\n",
    "    # 3. Train MaskGenerator and Classifier\n",
    "    # 4. Use Meta-Learning to train the KB, and KBClassifier \n",
    "    NUM_DAYS = 16\n",
    "    NUM_SYMBOLS = 20\n",
    "\n",
    "    days_accs = np.zeros((4, NUM_DAYS-1))\n",
    "    days_bwt = np.zeros((4, NUM_DAYS - 1))\n",
    "\n",
    "    NS1=[i for i in range(0, 308)]\n",
    "\n",
    "    # Only those symbols were chosen, which had data of all days in them\n",
    "    # Note this is different from filling missing values in original data frame\n",
    "    # For that, mean for that symbol on that day was used to fill NaN values.\n",
    "    NS1=list(set(NS1)-set([4, 5, 7, 8, 9, 11, 12, 13, 14, 16, 21, 27, 28, 36, 39, 40, 41, 42, 43, 45, 50, 52, 53, 57, 59, 61, 63, 65, 66, 67, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 87, 88, 89, 92, 93, 98, 99, 101, 104, 106, 107, 108, 112, 113, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 157, 160, 162, 163, 165, 166, 168, 171, 173, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 213, 218, 220, 222, 224, 226, 237, 238, 240, 241, 245, 247, 249, 250, 253, 254, 257, 258, 259, 262, 263, 267, 268, 271, 273, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 288, 293, 298, 299, 300, 304, 306, 307]))\n",
    "    NS=NS1[0:NUM_SYMBOLS]\n",
    "    NUM_SYMBOLS = len(NS)\n",
    "\n",
    "\n",
    "    all_freqs = pickle.load(open('market_cl_freqs.pkl','rb')) \n",
    "\n",
    "    for day in range(NUM_DAYS-1): # Last Day only for testing?\n",
    "        task_accs = np.zeros((4, NUM_SYMBOLS, NUM_SYMBOLS)) \n",
    "        print(f'Begin Day {day}', priority = LogPriority.MEDIUM)\n",
    "        task_num=0\n",
    "        for task_id in NS:\n",
    "            train_dl.dataset.set_day(day)\n",
    "            train_dl.dataset.set_symbol(task_id)\n",
    "\n",
    "            freqs = all_freqs[day][task_id]\n",
    "            criterions = [None, None, None, None]\n",
    "            for l in freqs:\n",
    "                freq = freqs[l]\n",
    "                print([v/(sum(freq.values())) for v in freq.values()])\n",
    "                class_weights = [1/f if f != 0 else 100 for f in freq.values()]\n",
    "                class_weights = torch.tensor([x/sum(class_weights) for x in class_weights], device = device)\n",
    "                criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "                criterions[l] = criterion\n",
    "\n",
    "            print('Frequencies for Current Day: ', freqs)\n",
    "            # Step 1: First train the feature extractor on task \n",
    "            # '''\n",
    "            import time\n",
    "            start_time=time.time()\n",
    "            print(f'Training Feature Extractor on task id {task_id}')\n",
    "            loss, acc = train_fe(model, train_dl, criterions, task_id, device = device, lr = float(experimenter.config.TRAINING.LR.FE), num_epochs = experimenter.config.TRAINING.EPOCHS.FE)\n",
    "            print(f'Feature Extraction Train Loss {loss} & Accuracy: {acc}')\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            print('\\n---------------------------------------------------------\\n')\n",
    "\n",
    "            # Step 2: Now if task is 0, train the KB without Meta-Learning\n",
    "\n",
    "            if baseline ==4:\n",
    "                start_time = time.time()\n",
    "                print(f'Training Initial Knowledge Base Weights')\n",
    "                loss, acc = train_kb_nonmeta_baseline(model, train_dl, criterions, task_id,\n",
    "                                             lr=float(experimenter.config.TRAINING.LR.INIT_KB), device=device,\n",
    "                                             num_epochs=experimenter.config.TRAINING.EPOCHS.INIT_KB)\n",
    "                print(f'Initial KB Validation Loss {loss} & Accuracy: {acc}')\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                print('\\n---------------------------------------------------------\\n')\n",
    "\n",
    "            if (task_id == 0 and day==0 and baseline!=4) or baseline==1 or baseline==3:\n",
    "                start_time = time.time()\n",
    "                print(f'Training Initial Knowledge Base Weights')\n",
    "                loss, acc = train_kb_nonmeta(model, train_dl, criterions, task_id, lr = float(experimenter.config.TRAINING.LR.INIT_KB), device = device, num_epochs = experimenter.config.TRAINING.EPOCHS.INIT_KB)\n",
    "                print(f'Initial KB Validation Loss {loss} & Accuracy: {acc}')\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                print('\\n---------------------------------------------------------\\n')\n",
    "            if baseline == 0 or baseline == 3:\n",
    "                # Step 3: Now train MaskGenerator and Classifier\n",
    "                start_time = time.time()\n",
    "                print(f'Training Mask Generator and Classifier')\n",
    "                loss, acc = train_mg_n_clf(model, train_dl, criterions, task_id, lr = float(experimenter.config.TRAINING.LR.MG_N_C), device = device, num_epochs = experimenter.config.TRAINING.EPOCHS.MG_N_C)\n",
    "                print(f'Mask Generation and Classifier Training Loss {loss} & Accuracy: {acc}')\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                print('\\n---------------------------------------------------------\\n')\n",
    "\n",
    "            if baseline == 0 or baseline == 2:\n",
    "                # '''\n",
    "                print('Updating KB')\n",
    "                print('Before',sum([x.sum() for x in model.kb[0].parameters()]))\n",
    "                start_time = time.time()\n",
    "                update_kb(model, train_dl, train_dl, criterions, task_id, device=device)\n",
    "                print(f'Update KB {loss} & Accuracy: {acc}')\n",
    "                print('After',sum([x.sum() for x in model.kb[0].parameters()]))\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                print('\\n---------------------------------------------------------\\n')\n",
    "                # '''\n",
    "\n",
    "            if baseline == 0 or baseline == 3:\n",
    "                # Stage 5: Fine-Tune Mask Generator and Final Classifier\n",
    "                print(f'Fine-Tune Mask Generator and Classifier')\n",
    "                start_time = time.time()\n",
    "                train_mg_n_clf(model, train_dl, criterions, task_id, lr = float(experimenter.config.TRAINING.LR.FINETUNE_MG_N_C), num_epochs = experimenter.config.TRAINING.EPOCHS.FINETUNE_MG_N_C, device = device)\n",
    "                print(f'Fine-Tuning Mask Generation and Classifier Training Loss {loss} & Accuracy: {acc}')\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "                print('\\n---------------------------------------------------------\\n')\n",
    "\n",
    "            # Needed only in case sup sup model is to be used.\n",
    "            model.cache_masks()\n",
    "\n",
    "            # Now report the numbers for task = 0 upto task = task_id\n",
    "            start_time = time.time()\n",
    "            print(f'Evaluating on task {task_id}\\n', priority = LogPriority.MEDIUM)\n",
    "            avg_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "            for t_id in range(task_id,task_id+1):\n",
    "                train_dl.dataset.set_day(day+1)\n",
    "                train_dl.dataset.set_symbol(t_id)\n",
    "                loss, acc, f1s = test_loop(model, train_dl, t_id, criterions, device = device)\n",
    "                print(f'Testing Loss for task = {t_id}: {loss}', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS )\n",
    "                print(f'Testing Accuracy for task = {t_id}: {acc}', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS)\n",
    "                print(f'Testing F1s for task = {t_id}: {f1s}\\n', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS)\n",
    "                \n",
    "                avg_loss += loss\n",
    "                for i, ac in acc.items():\n",
    "                    task_accs[i][task_num][task_num] = ac\n",
    "            task_num=task_num+1\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        print(f'Day {day} complete', priority = LogPriority.STATS)\n",
    "\n",
    "        # Now report the numbers for task = 0 upto task = task_id\n",
    "        start_time = time.time()\n",
    "        print(f'Evaluating on tasks {0} to {task_id}\\n', priority = LogPriority.MEDIUM)\n",
    "        avg_loss = 0\n",
    "        t_num=task_num-1\n",
    "        task_num=0\n",
    "        for t_id in NS:\n",
    "            # Set day and symbol for testing\n",
    "            train_dl.dataset.set_day(day+1)\n",
    "            train_dl.dataset.set_symbol(t_id)\n",
    "            loss, acc, f1s = test_loop(model, train_dl, t_id, criterions, device = device)\n",
    "            print(f'Testing Loss for task = {t_id}: {loss}', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS )\n",
    "            print(f'Testing Accuracy for task = {t_id}: {acc}', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS)\n",
    "            print(f'Testing F1s for task = {t_id}: {f1s}\\n', priority = LogPriority.MEDIUM if task_id != experimenter.cfg.DATASET.NUM_TASKS-1 else LogPriority.STATS)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            for i, ac in acc.items():\n",
    "                task_accs[i][t_num][task_num] = ac\n",
    "            task_num=task_num+1\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        base_path = './'\n",
    "        pathS = os.path.join(base_path,f'MARK_MODELS/MARKET/{baseline}_{iter}_model_{day}.pt')\n",
    "        os.makedirs(os.path.split(pathS)[0], exist_ok=True)\n",
    "        torch.save(model, pathS)\n",
    "\n",
    "        for i in range(4):\n",
    "            print(f'Label {i}', priority = LogPriority.STATS)\n",
    "            print(task_accs[i])\n",
    "            ta = task_accs[i]\n",
    "            avg_acc = ta[-1].mean()\n",
    "            bwt = sum(ta[-1]-np.diag(ta))/ (ta.shape[1]-1)\n",
    "            print(f'Average Accuracy: {avg_acc}', priority = LogPriority.STATS)\n",
    "            print(f'BWT: {bwt}', priority = LogPriority.STATS)\n",
    "            days_accs[i][day] = avg_acc\n",
    "            days_bwt[i][day] = bwt\n",
    "    # Note this returns only for day 53 and 3rd label\n",
    "    return days_accs.mean(axis=1), days_bwt.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model and print the accuracies and BWT\n",
    "acc, bwt = train(model, criterion, train_dl, experimenter, device, baseline = 0, iter = 0)\n",
    "print(f'Accuracies: {acc}', priority = LogPriority.STATS)\n",
    "print(f'BWTS: {bwt}', priority = LogPriority.STATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributors:\n",
    "[Prerna Agarwal](https://github.com/prerna-agarwal-iitd)\n",
    "<br>\n",
    "[Pranjal Aggarwal](https://github.com/Pranjal2041/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "318f56b2e3b576cc5b4f1eea640f05660eda47842fd3e1596fbcc23ab618e823"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pygpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
